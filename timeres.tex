\chapter{Temporal resolution of pulse detection}
\label{ch:timeres}

In the previous chapter we studied the performance of filters, but we did not
define an algorithm to search for signals. Neither we will do it here, we will
skip it and measure the temporal localization precision once we know there is a
signal.

To this end, we make a simulation where each ``event'' contains only one signal
at a known position. In principle we could use the LNGS data
(\autoref{sec:lngsdata}), but we do not know the jitter of the trigger pulse
and we may reach a temporal resolution below the sampling period, while in the
simulation we have the exact actual temporal location of signals.

\section{Event generation}

Each event is the sum of a signal and a noise waveform. We do not add a
baseline, so the noise has mean zero and the signals taper down to zero. The
signals are negative. We use the same scale of the LNGS data; it actually does
not matter because we are not simulating digitalization.

\subsection{Signal}
\label{sec:toysignal}

We use the trigger-aligned template from \SI1{PE} laser pulses in Tile~57, see
\autoref{sec:cctemplate} and \autoref{fig:template}.

The template is at \SI1{GSa/s}, but the simulation is at \SI{125}{MSa/s}, which
is the sampling frequency planned for the DarkSide20k digitizers. We have to
downsample the template and shift its temporal position continuously instead of
by $(\SI1{GHz})^{-1} = \SI1{ns}$ steps. Given the continuous temporal position
where we want to place the template, we round it by excess and defect to the
\SI{1}{ns} timebase. Then we downsample the template by averaging samples in
groups of~8, once with the groups aligned to the floor rounded position, once
with the ceiling rounded one. Finally we interpolate linearly between the two
downsampled templates. \autoref{fig:interptempl} shows a series of waveforms
obtained in this way.

Downsampling with an average is not the best antialiasing filter doable, but it
should be fine for our application.

In each simulation event we vary the amplitude of the signal by an additive
Gaussian random variable, which has the standard deviation observed in the LNGS
data, \SI{2.9}\%. This value is obtained by computing the difference in
quadrature between the ``quantile standard deviations'' (see
\autoref{sec:fingerplot}) of the \SI{0}{PE} and \SI{1}{PE} peaks in the
fingerplot done with a \SI{1.5}{\micro s} average, the same used for the
template in \autoref{sec:cctemplate}, and dividing it by the median of the
\SI{1}{PE} peak.

\begin{figure}
    
    \widecenter{\includempl{figinterptempl}}

    \figcaption{interptempl}{The signal template downsampled from \SI1{GSa/s}
    to \SI{125}{MSa/s} and translated continuously instead of by discrete steps
    with linear interpolation.}

\end{figure}

\subsection{Noise}

We used three different kinds of noise: Gaussian white noise; noise sampled
from the LNGS data; noise sampled from the Proto0 data.

The white noise is generated in the simulation. The LNGS noise is copied from
the pre-trigger region, with the same selections explained in
\autoref{sec:snrdata}, i.e.\ ignoring any event with a sample less than 700.
The Proto0 noise is copied from the whole events without selection. The
detector is always Tile~57, as for the signal template. A persistence plot of
the Proto0 data is shown in \autoref{fig:hist2dtile155759}. The spectra are
shown in \autoref{sec:spectrum}, the autocorrelations in
\autoref{fig:autocorrlngs}.

The filter applied to the LNGS noise data may not be strict enough, letting
spurious \SI{1}{PE} pulses appear in the simulation. We will use robust
measures (quantiles), so outliers caused by this or any other transient problem
will not spoil the results.

When taking the noise for multiple simulation events from the same data event,
we skip \SI{1}{\micro s} between each chunk in the data, just be safe in
avoiding correlations between the simulation events.

The noise obtained from data is normalized to the variance required by the
simulation. We normalize the variance separately for each \emph{data} event.

We downsample the noise in the same way as the signal, by averaging nearby
samples. Both noises have spectra that go down with frequency, so this crappy
antialiasing should be sufficient. The normalization to the desired variance is
done after downsampling. The order matters because downsampling with averaging
reduces the variance of the noise. See \autoref{fig:noise}. The Proto0 noise
data is already at \SI{125}{MSa/s} and so did not require downsampling for most
of the simulations.

\begin{figure}
    
    \widecenter{\includempl{fignoise}}
    
    \figcaption{noise}{The LNGS and Proto0 noise at the original sampling
    frequency (normalized to unit variance) and downsampled.}

\end{figure}

\subsection{Event layout}

Each event is the sum of a noise waveform and a shorter signal waveform. Before
the beginning of the signal there is a noise-only region long enough for the
filters to be in a stationary state when they reach the signal; in particular
its length is the highest filter length parameter used in the simulation,
\SI{2048}{ns}, plus \SI{256}{ns}.

The simulation is repeated for various signal to noise ratios (SNR). We define
the SNR as follows: the peak height relative to the baseline of the original
\SI1{GSa/s} signal template over the noise standard deviation.

The reason we consider the signal amplitude at \SI1{GSa/s} instead of at the
actual sampling frequency of the simulation is that downsampling reduces the
peak height. For convenience, we want to keep the definition of signal height
comparable at different sampling frequencies. This will be clearer later when
we do the comparison.

\marginpar{I should use the peak height, averaged over continuous positioning,
at the simulation sampling frequency, instead of at \SI{1}{GSa/s}. Then the
comparisons at different sampling frequencies would need to adjust both for the
reduction of the noise variance and of the peak height, but the definition of
SNR would be the standard one.}

Simulations with different SNR differ only in the multiplicative constant of
the noise, so we used exactly the same noise and signal arrays for each SNR to
speed up the code. This means that there is no random variation between results
obtained at different SNR (or with different filters), keep this in mind when
the smoothness of some curves would seem to suggest that the Monte Carlo error
is negligible.

\autoref{fig:toyevent} shows a complete example event.

\begin{figure}
    
    \widecenter{\includempl{figtoyevent}}
    
    \figcaption{toyevent}{A simulation event. The dots are the minima of the
    filters output. The minima are searched in the shaded region only; this
    makes no difference with high enough SNR like in this example, but in the
    limit SNR = 0 the minimum fluctuates around uniformly: the search range
    sets the endpoints of this distribution.}

\end{figure}

\section{Temporal localization}

We run the three filters described in \autoref{sec:filters} (moving
average, exponential moving average, cross correlation), then take the minimum
(the signals are negative) of the filtered waveform as the location of the
signal. We also take the minimum of the unfiltered waveform as baseline
comparison.

The minimum of the filter output occurs in some sense later than the signal
location, this is not a problem since the choice of the point of the signal to
be taken as reference is already arbitrary.

To make the template for the cross correlation filter, we first truncate it
as described in \autoref{sec:cctemplate}, and then downsample it in the same
way we downsample the signal template and the noise.

\begin{figure}
    
    \widecenter{\includempl{figtoyfilttempl}}
    
    \figcaption{toyfilttempl}{Some cross correlation filter templates for
    different lengths. It may appear strange that the endpoint on the left has
    a different height than the endpoint on the right for a given template,
    since we choose the truncation to maximize the norm; it happens because we
    downsample \emph{after} truncation.}

\end{figure}

To allow for a localization eventually more precise than the sampling timebase,
we interpolate the minimum sample and its first neighbors with a parabola. We
also try upsampling the waveform to \SI{1}{GSa/s} (with sample repetition)
prior to filtering to check if it improves performance.

\subsection{Resolution}

We repeat the simulation for 1000 events for each filter, filter length
parameter, and SNR in some range, using the Proto0 noise.
\autoref{fig:lochist} shows the histograms of the temporal localization for
all filters for a choice of SNR and filter length. The signal template position
varies continuously and uniformly within one sampling step.

\begin{figure}
    
    \widecenter{\includempl{figlochist}}
    
    \figcaption{lochist}{Histograms of the temporal localization error, i.e.\
    the difference between the filter output minimum and the signal template
    start, translated to have zero median, for a choice of SNR and filters
    length. The error bars mark the \SI{16}\% and the \SI{84}\% quantiles. As
    definition of temporal resolution we take half the distance between those
    quantiles. The sampling step is \SI{8}{ns}.}

\end{figure}

We see that the distribution of localizations can be non-Gaussian, so to
quantify the resolution we use, instead of the standard deviation, half the
distance between the \SI{16}\% and \SI{84}\% quantiles, which is equivalent to
a standard deviation for a Gaussian, but gives a meaningful measure for the
width of the distribution even when it is highly skewed or with heavy tails.

\autoref{fig:rescurve} shows the temporal resolution thus defined for each
filter, filter length, and SNR. The exponential moving average has a
consistently poor performance compared to the other filters. The
cross-correlation filter is the best one, with performance improving with
length, and at a length of 96 samples (\SI{768}{ns}) is already practically
optimal. The moving average can get close to the cross-correlation filter by
choosing appropriately the number of samples.

\begin{figure}
    
    \widecenter{\includempl{figrescurve}}
    
    \figcaption{rescurve}{Pulse detection temporal resolution for a range of
    SNR and filter lengths. The shaded region marks the sampling
    step~\SI{8}{ns}. The right endpoint of the cross correlation filter curves
    is at~\SI{2.6}{ns}.}

\end{figure}

The online processing of the PDM output in the experiment will be done in two
steps: the digitizers must find the signals, then send them to the front end
processors (FEPs) for further analysis. The computational resources of the
digitizers are limited compared to the FEPs.

The exponential moving average can be surely implemented on the digitizers. The
cross-correlation with 64 samples could probably be done on the digitizers
since a similar computation was implemented in another study.

The FEPs can and should probably use the best filter, so they would run a long
cross-correlation filter. The temporal resolution matters in the FEPs but
probably not in the digitizers, in the latter case it is just a generic measure
of performance.

Thus out of all the temporal resolution curves the ones that matter are:
%
\begin{itemize}
    %
    \item the best we can do with the exponential moving average and moving
    average;
    %
    \item the long cross-correlation filters;
    %
    \item the 64 samples cross-correlation filter.
    %
\end{itemize}
%
We plotted these curves together in \autoref{fig:rescomp}, adding some
curves done with the LNGS and white noises. Note that a different noise
spectrum makes a large difference at low SNR. We also plot a curve computed
with upsampling; it does not improve significantly the performance.

\begin{figure}
    
    \widecenter{\includempl{figrescomp}}
    
    \figcaption{rescomp}{Pulse detection temporal resolution vs.\ SNR for
    various filters. The shaded region marks the sampling step~\SI{8}{ns}. The
    hatched band is the interval of SNR observed in Proto0; the vertical line
    is the SNR in the LNGS data, after downsampling to~\SI{125}{MSa/s}. Where
    not specified, the noise is from Proto0.}

\end{figure}

\section{Data reduction}

We said that the digitizers must find signals in the waveform stream and send
them to the FEPs for further processing. The bandwidth of the connection
between the digitizers and the FEPs happens to be a bottleneck. Two possible
ways of reducing the amount of transmitted data are keeping only the minimum
number of samples for each signals, and reducing the sampling frequency. Both
have an effect on the temporal resolution, which we assess here.

\subsection{Waveform truncation}

We repeat the simulation, but this time we use only a fixed smaller number of
samples in each event to compute the filter output. We call this selection of
samples ``window''. On the window we run only a long cross-correlation filter
since that is what would be done on the FEPs. As past and future boundary
condition we use zero. We evaluate the filter even after the sample window end
because the window can be shorter than the filter.

While the length of the window is fixed, the placement is not fixed relative to
the true signal location. Instead we use the temporal localization with another
filter feasible on the digitizers, calibrated to have the median aligned to the
beginning of the signal template. The window then extends a given number of
samples to the left and to the right of this localization.

\autoref{fig:windowevent} shows this procedure graphically for a single
event. \autoref{fig:windowtempres} shows the temporal resolution versus
unfiltered SNR curves for various choices of window length, noise, and filter
used to align the window, where for reasons of computation time the latter was
computed at a fixed SNR that does not follow the value on the x-axis.

\begin{figure}
    
    \widecenter{\includempl{figwindowevent}}
    
    \figcaption{windowevent}{Left panel: a simulation event filtered with the
    exponential moving average. Right panel: the same event filtered with a
    long cross correlation filter, both using the whole waveform and using only
    the samples in the shaded window, which is centered using the localization
    from the filter in the left panel.}

\end{figure}

\begin{figure}
    \widecenter{\includempl{figwindowtempres}}
    
    \figcaption{windowtempres}{Pulse detection temporal resolution with a long
    cross correlation filter applied only on a short window of samples centered
    using a shorter cross correlation filter (left panels) or an exponential
    moving average (right panels). The various curves correspond to different
    window lengths, while the black dots are the resolution without windowing.}

\end{figure}

By looking at \autoref{fig:windowtempres}, we conclude that probably it is
sufficient to save \SI1{\micro s} of waveform to obtain practically the same
temporal resolution achievable without windowing.

What is missing in this study is that we did not try to optimize the left/right
balance of the window, and that as said above the unwindowed localization used
to align the windows is done at a mismatched SNR. The first problem can only
worsen the temporal resolution obtained, so it is conservative; the second is
conservative assuming that our choice of SNR, 2.5, is lower that what we expect
in the detector.

\subsection{Downsampling}

Another way of reducing the data throughput is downsampling. In
\autoref{fig:tempresdowns} we show the temporal resolution achieved with a long
cross correlation filter at different sampling frequencies. We observe that
downsampling by a factor of 2 from \SI{125}{MSa/s} to \SI{62}{MSa/s} maintains
almost the same temporal resolution, while going to \SI{31}{MSa/s} lowers it
visibly.

When downsampling a waveform, the variance of the noise is reduced. At each
sampling frequency the simulation sets the SNR looking at the standard
deviation of the already downsampled noise, so the SNR scales are off by the
factor of the noise amplitude reduction. Think about it this way: to make the
simulations comparable, we should start by a ``master simulation'' at
\SI{1}{GSa/s}, then downsample it various times. Our code does not implement
this and repeats the simulation from scratch at each sampling frequency,
renormalizing the downsampled noise to unitary variance, so it gets an higher
noise. Thus the SNR of the downsampled simulations is ``actually lower'' than
what they declare.

\begin{figure}
    
    \widecenter{\includempl{figtempresdowns}}
    
    \figcaption{tempresdowns}{Pulse detection temporal resolution at different
    sampling frequencies with a cross correlation filter with template length
    \SI{2048}{ns}. The SNR scale is at \SI{125}{MSa/s}; curves for different
    sampling frequencies are rescaled horizontally by the factor written in the
    legend to account for the noise variance reduction with downsampling.}

\end{figure}

Since we are downsampling we also need to check if we lose signal to noise
ratio in the filter output. In \autoref{tab:filtsnrdowns} we report the
ratio between SNR after and before filtering. It does not appear to change.

\marginpar{Add digitalization to the simulation and make a plot like the one
for the sampling frequency but varying the number of bits at \SI{125}{MSa/s}.
Don't use the number of bits, use the ratio signal peak over digit which is
well defined.}

\begin{table}
    \centering
    
    \begin{tabular}{c*4S[table-format=1.1]}
        \toprule
               & \multicolumn4c{SNR after over before filtering} \\
        \cmidrule(l){2-5}
         Noise & {\SI{1}{GSa/s}} & {\SI{125}{MSa/s}} & {\SI{62.5}{MSa/s}} & {\SI{31.2}{MSa/s}} \\
        \midrule
        Proto0 &               &             3.3 &              3.3 &              3.3 \\
          LNGS &           5.6 &             5.5 &              5.7 &              6.0 \\
         White &           4.3 &             4.3 &              4.2 &              4.2 \\
        \bottomrule
    \end{tabular}
    
    \caption{\label{tab:filtsnrdowns} Ratio of SNR after over before filtering
    with a cross correlation filter with template length \SI{2048}{ns}. The
    \SI{125}{MSa/s} column contains the actual SNR ratios of the simulations,
    while the values for the other sampling frequencies are divided by the
    noise standard deviation reduction with downsampling relative to
    \SI{125}{MSa/s} to make them comparable.}
    
\end{table}
